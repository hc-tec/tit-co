# cocoyaxi协程调度

使用栗子

```c++
  go([&]() {
      bool r = ev.wait(50);
      LOG << "f2() r: " << r;
  });
```


## 调度解析

最外层所用到的go实际上是一个模板函数，它提供了一些重载函数方便开发者传递需要的函数体，目的是将各种可调用的对象进一步封装为Closure对象，统一使用。

```c++
void go(Closure* cb);

template<typename F>
inline void go(F&& f) {
    go(new_closure(std::forward<F>(f)));
}

template<typename F, typename P>
inline void go(F&& f, P&& p) {
    go(new_closure(std::forward<F>(f), std::forward<P>(p)));
}

template<typename F, typename T, typename P>
inline void go(F&& f, T* t, P&& p) {
    go(new_closure(std::forward<F>(f), t, std::forward<P>(p)));
}


// 栗子
go(f);             // void f();
go(f, 7);          // void f(int);
go(&T::f, &o);     // void T::f();    T o;
go(&T::f, &o, 3);  // void T::f(int); T o;

// lambda
go([](){
    LOG << "hello co";
});

// std::function
std::function<void()> x(std::bind(f, 7));
go(x);
go(&x); // Ensure that x is alive when the coroutine is running.

```



这些go函数最终都会调用下面这个go函数

```c++
void go(Closure* cb);
```

它的实现在scheduler.cc中的270\~272行

```c++
void go(Closure* cb) {
    ((SchedulerImpl*) scheduler_manager()->next_scheduler())->add_new_task(cb);
}
```

scheduler\_manager()函数会返回一个静态的SchedulerManager\*对象，它是一个调度管理器，根据CPU核数创建相应数量的调度器（scheduler）。

```c++
inline SchedulerManager* scheduler_manager() {
    static auto ksm = co::new_static<SchedulerManager>();
    return ksm;
}
```

之后通过next\_scheduler()方法得到一个scheduler\*，这里通过特定的算法从vector中获取到调度器。

```c++
  Scheduler* next_scheduler() {
      if (_s != (uint32)-1) return _scheds[atomic_inc(&_n) & _s];
      uint32 n = atomic_inc(&_n);
      if (n <= ~_r) return _scheds[n % _scheds.size()]; // n <= (2^32 - 1 - r)
      return _scheds[now::us() % _scheds.size()];
  }
```

Scheduler是一个调度器对象，实际上只是一个基类，提供了go的几种重载实现。实际工作的是SchedulerImpl对象，它继承Scheduler，（这里类似于PImpl惯用法）。

SchedulerImpl内部管理了许多对象。

*   TaskManager用于管理各种非定时任务，含有新任务队列和就绪协程任务队列。

    ```c++
      co::vector<Closure*> _new_tasks; // 新任务列表，新任务指的是刚被创建，没有yield中断过的任务
      co::vector<Coroutine*> _ready_tasks; // 就绪任务指的是已被创建，可能还运行了一段时间，中间被yield中断过的任务
    ```

*   TimerManager用于管理定时任务，其内部使用std::multimap作为任务容器，可以按照到期时间自动排序定时任务。

*   Copool是一个协程池，维护了协程表与协程id列表。

*   Epoll对象将协程与IO多路复用结合起来。通过Epoll实现了具体的调度算法。

*   Stack\* \_stack是一个栈列表，Stack是一个栈结构体。

    ```c++
    struct Stack {
        char* p;       // stack pointer 
        char* top;     // stack top
        Coroutine* co; // coroutine owns this stack
    };
    ```

*   Coroutine\* \_main\_co; 保存了主协程。

*   Coroutine\* \_running; 保存了当前正在运行的协程。

*   SyncEvent 是一个同步事件。



一个Scheduler其实就代表着一个线程，在初始化时调用start()方法会开启一个线程，这个线程将负责调度工作。

```c++
// start the scheduler thread
void start() { Thread(&SchedulerImpl::loop, this).detach(); }


void SchedulerImpl::loop() {
    gSched = this;
    co::vector<Closure*> new_tasks;
    co::vector<Coroutine*> ready_tasks;

    while (!_stop) {
      ...
    }
}

```



有了这些知识，咱们回过去继续看go的实现

```c++
void go(Closure* cb) {
    ((SchedulerImpl*) scheduler_manager()->next_scheduler())->add_new_task(cb);
}
```

可以看到，go一个函数，其实就是把这个函数放到任务队列中，等待调度。



知道任务如何被添加之后，接下来咱们看看协程如何进行切换。

Coroutine结构体的实现细节：

```c++
struct Coroutine {
    Coroutine() { memset(this, 0, sizeof(*this)); }
    ~Coroutine() { it.~timer_id_t(); stack.~fastream(); }

    uint32 id;         // coroutine id
    uint8 state;       // coroutine state
    uint8 sid;         // stack id
    uint16 _00_;       // reserved
    void* waitx;       // wait info
    tb_context_t ctx;  // context, a pointer points to the stack bottom

    // for saving stack data of this coroutine
    union { fastream stack; char _dummy1[sizeof(fastream)]; };
    union { timer_id_t it;  char _dummy2[sizeof(timer_id_t)]; };

    // Once the coroutine starts, we no longer need the cb, and it can
    // be used to store the Scheduler pointer.
    union {
        Closure* cb;   // coroutine function
        Scheduler* s;  // scheduler this coroutine runs in
    };
};
```



一个协程有四种状态，转化关系以及声明如下：

```c++
/**
 * coroutine state 
 *   - The state is used to implement co::Event.
 *
 *                    co::Event::wait()
 *          st_init ---------------------> st_wait
 *             ^                              |
 *             |                              |
 *    resume() |                              |
 *             |            timeout           |
 *             ^<---------------------------- v
 *             |                              |
 *             |                              |
 *             |                              |
 *             ^       co::Event::signal()    |
 *         st_ready <------------------------ v
 */
enum co_state_t : uint8 {
    st_init = 0,     // initial state
    st_wait = 1,     // wait for an event
    st_ready = 2,    // ready to resume
    st_timeout = 4,  // timeout
};
```



协程被调度器调度时具体发生的过程

```c++
/*
 *  scheduler thread:
 *
 *    resume(co) -> jump(co->ctx, main_co)
 *       ^             |
 *       |             v
 *  jump(main_co)  main_func(from): from.priv == main_co
 *    yield()          |
 *       |             v
 *       <-------- co->cb->run():  run on _stack
 */
```



协程resume（恢复）时：

协程的resume由调度器(SchedulerImpl)实现，参数为Coroutine\*对象。



```c++
void SchedulerImpl::resume(Coroutine* co) {
    tb_context_from_t from;
    // 获取到栈，这个栈不一定为待恢复的协程栈
    Stack* s = &_stack[co->sid];
    _running = co;
    // 如果栈为空，重新分配空间
    if (s->p == 0) {
        s->p = (char*) co::alloc(_stack_size);
        s->top = s->p + _stack_size;
        s->co = co;
    }
    
    if (co->ctx == 0) {
        // resume new coroutine
        // 如果拿到的栈并不是待恢复的协程栈
        // 那么就将此栈从共享栈中移入到拥有此栈的协程的私有栈中
        // 这个栈所有权就归待恢复协程栈
        if (s->co != co) { this->save_stack(s->co); s->co = co; }
        co->ctx = tb_context_make(s->p, _stack_size, main_func);
        CO_DBG_LOG << "resume new co: " << co << " id: " << co->id;
        // 正式进行协程切换
        // 跳转到main_func方法中执行
        from = tb_context_jump(co->ctx, _main_co); // jump to main_func(from):  from.priv == _main_co

    } else {
        // remove timer before resume the coroutine
        if (co->it != _timer_mgr.end()) {
            CO_DBG_LOG << "del timer: " << co->it;
            _timer_mgr.del_timer(co->it);
            co->it = _timer_mgr.end();
        }

        // resume suspended coroutine
        CO_DBG_LOG << "resume co: " << co << ", id: " <<  co->id << ", stack: " << co->stack.size();
        if (s->co != co) {
            this->save_stack(s->co);
            CHECK(s->top == (char*)co->ctx + co->stack.size());
            memcpy(co->ctx, co->stack.data(), co->stack.size()); // restore stack data
            s->co = co;
        }
        from = tb_context_jump(co->ctx, _main_co); // jump back to where the user called yiled()
    }
    
    if (from.priv) {
        // 协程中断
        // yield() was called in the coroutine, update context for it
        assert(_running == from.priv);
        _running->ctx = from.ctx;
        CO_DBG_LOG << "yield co: " << _running << " id: " << _running->id;
    } else {
        // 协程完全调用完毕，回收
        // the coroutine has terminated, recycle it
        this->recycle();
    }
}

void SchedulerImpl::main_func(tb_context_from_t from) {
    ((Coroutine*)from.priv)->ctx = from.ctx;
    // 一个协程代表着一个函数调用
    // 函数调用彻底结束后，即 return 掉之后
    // 协程应该回收
    gSched->running()->cb->run(); // run the coroutine function
    // 执行到这里说明函数已经return，完全运行结束
    tb_context_jump(from.ctx, 0); // jump back to the from context
}

```




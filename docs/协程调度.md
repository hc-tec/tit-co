# cocoyaxi 协程调度

## cocoyaxi库简介

CO 是一个优雅、高效的 C++ 基础库，支持 Linux, Windows 与 Mac 等平台，它实现了类似 golang 的协程、基于协程的网络编程框架、命令行参数与配置文件解析库、高性能日志库、单元测试框架、JSON 库等一系列高质量的基础组件。

## 官方文档中的协程介绍

<https://idealvin.github.io/cn/co/coroutine/>

### 基本概念

*   协程是运行于线程中的轻量级调度单位。

*   协程之于线程，类似于线程之于进程。

*   一个进程中可以存在多个线程，一个线程中可以存在多个协程。

*   协程所在的线程一般被称为**调度线程**。

*   协程发生 io 阻塞或调用 sleep 等操作时，调度线程会挂起此协程。

*   协程挂起时，调度线程会切换到其他等待中的协程运行。

*   协程的切换是在用户态进行的，比线程间的切换更快。

协程非常适合写网络程序，可以实现**同步的编程方式**，不需要异步回调，大大减轻了程序员的思想负担。

co 协程库实现的是一种类似 [golang](https://github.com/golang/go/ "golang") 的协程，有如下特性：

*   支持多线程调度，默认线程数为系统 CPU 核数。

*   共享栈，同一线程中的协程共用若干个栈(大小默认为 1MB)，内存占用低，Linux 上的测试显示 1000 万协程只用了 2.8G 内存(仅供参考)。

*   协程创建后，始终在同一个线程中运行，而不会切换到其他线程。

*   各协程之间为平级关系，可以在任何地方(包括在协程中)创建新的协程。

co 协程库在 linux, mac, windows 等平台，分别基于 [epoll](http://man7.org/linux/man-pages/man7/epoll.7.html "epoll"), [kqueue](https://man.openbsd.org/kqueue.2 "kqueue"), [iocp](https://docs.microsoft.com/en-us/windows/win32/fileio/i-o-completion-ports "iocp") 实现。

co 协程库中 context 切换相关的代码，取自 [ruki](https://github.com/waruqi "ruki") 的 [tbox](https://github.com/tboox/tbox/ "tbox")，而 tbox 则参考了 [boost](https://www.boost.org/doc/libs/1_70_0/libs/context/doc/html/index.html "boost") 的实现，在此表示感谢！

### 协程用法：go

```c++
void go(Closure* cb);
template<typename F> void go(F&& f);
template<typename F, typename P> void go(F&& f, P&& p);
template<typename F, typename T, typename P> void go(F&& f, T* t, P&& p);
```

### 代码示例

```c++
go(f);             // void f();
go(f, 7);          // void f(int);
go(&T::f, &o);     // void T::f();    T o;
go(&T::f, &o, 3);  // void T::f(int); T o;

// lambda
go([](){
    LOG << "hello co";
});

// std::function
std::function<void()> x(std::bind(f, 7));
go(x);
go(&x); // Ensure that x is alive when the coroutine is running.
```

## 什么是协程调度

这里的调度应该理解为类似于操作系统对进程、线程的调度。

在cocoyaxi库的协程调度模型中，一个程序可以含多个调度器，每个调度器对应一个线程。调度器内部包含了任务列表，go一个协程其实代表着协程被添加到任务列表中，并不会立即执行go的那个协程。以操作系统术语作为解释的话，目前的协程调度方式为**非抢占式调度**。

那么调度器在添加任务过程中具体做了哪些事情？以及被添加到任务列表中的任务在何时会被执行？接下来，我们从代码深入到cocoyaxi库的协程调度模块。

## 深入协程调度

### go一个函数，go一个协程

我们从协程被加入进调度列表前出发，也就是go函数出发。

```c++
void go(Closure* cb);
template<typename F> void go(F&& f);
template<typename F, typename P> void go(F&& f, P&& p);
template<typename F, typename T, typename P> void go(F&& f, T* t, P&& p);
```

Closure是一个泛化仿函数，2-4行代码所示的go模板函数的参数最终都会被封装为Closure，并调用第一个go函数。

比如说：

```c++
template<typename F>
inline void go(F&& f) {
    go(new_closure(std::forward<F>(f)));
}
```

接着，我们需要知道第一个go函数是如何实现的。

```c++
// co.h 29行
void go(Closure* cb);
```

在co.h中我们只能看到一个函数声明，继续跟踪会跳转到位于scheduler.cc中go具体实现。

```c++
void go(Closure* cb) {
    ((SchedulerImpl*) scheduler_manager()->next_scheduler())->add_new_task(cb);
}
```

这就是go函数的具体实现，意思是从调度管理器（SchedulerManager）中取出下一个调度器，调用调度器的add\_new\_task方法，将任务添加到此调度器的任务列表中。

之所以前面加了个 (SchedulerImpl\*)，是因为作者在Scheduler上使用继承的方式实现了PImpl惯用法。

继续一步步地拆解出来看

```c++
inline SchedulerManager* scheduler_manager() {
    static auto ksm = co::new_static<SchedulerManager>();
    return ksm;
}
```

scheduler\_manager() 内部以static方式提供了一个全局变量， 此变量类型为SchedulerManager 。

SchedulerManager 类功能是对调度器（Scheduler）创建和管理。

SchedulerManager 对调度器的创建体现在，SchedulerManager 的构造函数中会创建出相应个数的调度器，为这些调度器分配id和栈大小，调用调度器的start方法，并保存到vector中。开发者可以自由控制调度器创建的个数。见[协程 | Documents for CO(idealvin.github.io)](https://idealvin.github.io/cn/co/coroutine/#co_sched_num "协程 | Documents for CO(idealvin.github.io)")。

```c++
class SchedulerManager {
  public:
    SchedulerManager();
    ~SchedulerManager();
    ...
  private:
    co::vector<Scheduler*> _scheds;
    ...
};


SchedulerManager::SchedulerManager() {
    ...
    if (FLG_co_sched_num == 0 || FLG_co_sched_num > (uint32)os::cpunum()) 
      // 默认调度器数量为 CPU 核数
      FLG_co_sched_num = os::cpunum();
    if (FLG_co_stack_size == 0) 
      // 默认协程栈大小为 1M
      FLG_co_stack_size = 1024 * 1024;
      
    ...
    
    for (uint32 i = 0; i < FLG_co_sched_num; ++i) {
        // 提供 id 和 栈大小
        SchedulerImpl* s = new SchedulerImpl(i, FLG_co_sched_num, FLG_co_stack_size);
        // 调用start方法，启动一个线程，开启调度循环        
        s->start();
        _scheds.push_back(s);
    }
    is_active() = true;
}

```

SchedulerManager 对调度器的管理体现在，由SchedulerManager 决定任务应该被添加到哪一个调度器中。在其next\_scheduler()方法中能看到管理办法。

```c++
class SchedulerManager {

  public:
    Scheduler* next_scheduler() {
        if (_s != (uint32)-1) return _scheds[atomic_inc(&_n) & _s];
        uint32 n = atomic_inc(&_n);
        if (n <= ~_r) return _scheds[n % _scheds.size()]; // n <= (2^32 - 1 - r)
        return _scheds[now::us() % _scheds.size()];
    }
  
  private:
    co::vector<Scheduler*> _scheds;
    uint32 _n;  // index, initialized as -1
    uint32 _r;  // 2^32 % sched_num
    uint32 _s;  // _r = 0, _s = sched_num-1;  _r != 0, _s = -1;

};

```



```c++
void go(Closure* cb) {
    ((SchedulerImpl*) scheduler_manager()->next_scheduler())->add_new_task(cb);
}
```

回看代码((SchedulerImpl\*) scheduler\_manager()->next\_scheduler())，通过前文分析，可以清楚知道这里获取到了一个调度器指针，之后调用的是调度器的add\_new\_task方法，将任务添加到任务列表中。

```c++
class SchedulerImpl : public co::Scheduler {
 public:
   ...
    // add a new task will run in a coroutine later (thread-safe)
    void add_new_task(Closure* cb) {
        // 添加新任务
        _task_mgr.add_new_task(cb);
        // 唤醒 epoll
        _epoll->signal();
    }
   ...
 private:
    ...
    TaskManager _task_mgr;
    ...
} 
```

TaskManager 是一个任务管理器，内部维护了任务列表，并提供了添加任务和获取任务列表的方法。

```c++
class TaskManager {
  public:
    TaskManager() = default;
    ~TaskManager() = default;

    void add_new_task(Closure* cb) {
        ::MutexGuard g(_mtx);
        _new_tasks.push_back(cb);
    }

    void add_ready_task(Coroutine* co) {
        ::MutexGuard g(_mtx);
        _ready_tasks.push_back(co);
    }

    void get_all_tasks(
        co::vector<Closure*>& new_tasks,
        co::vector<Coroutine*>& ready_tasks
    ) {
        ::MutexGuard g(_mtx);
        if (!_new_tasks.empty()) _new_tasks.swap(new_tasks);
        if (!_ready_tasks.empty()) _ready_tasks.swap(ready_tasks);
    }
 
  private:
    ::Mutex _mtx;
    co::vector<Closure*> _new_tasks;
    co::vector<Coroutine*> _ready_tasks;
};
```

go函数执行到这一步它的使命就完成了。

不过到目前为之，我们了解到的有关调度器的原理还是很浅。

#### 解决了哪些问题？

从前文一步一步地挖掘中，我们清楚了go函数的功能—将函数转化为Closure对象，并将这个对象添加到任务列表中。

#### 还需要挖掘哪些问题？

*   上面的TaskManager类源码中，在成员变量部分可以发现其实内部包含了两个列表，一个叫\_new\_tasks，它是Closure\*列表，还有个叫\_ready\_tasks，它是Coroutine列表。为什么需要这样安排，用一个列表不行吗？如果不行，它们之间有什么区别呢？

*   协程的使用目前也还没有出现，协程时如何被创建出来的？泛化仿函数Closure就是一个协程吗？它与协程的关系是怎样的？

*   我们都知道协程和线程一样，都需要保存自己的栈，cocoyaxi库是如何做的呢？采用的是静态栈、拷贝栈、共享栈、还是虚拟栈呢？

*   调度循环是调度的核心，它的工作原理是什么？

接下来我们将从上面的问题出发，逐步拆解，得到答案。

### 两种任务

由上文的TaskManager引出两种任务，它们的类型不同，一种是Closure对象，本质上是一个函数，一种是Coroutine对象，它是协程本体。

#### 什么是new task

go出来的，刚被添加到任务列表，还**没有开始调度**的任务。

#### 什么是ready task

曾经调度过，执行过，不过由于各种原因（sleep、等待资源 等）导致yield，协程被挂起。当需要resume协程时，可以把协程添加到调度器的ready\_tasks列表中，等待调度。一句话，需要的资源已准备好，等待调度的协程。

在协程调度时，Closure对象会封装成一个协程，并以协程身份参与之后的调度。

已经解释了两种任务的区别，还不能解释为什么需要这样设计的原因。

不难发现，我们完全可以一个任务列表来保存协程。怎么做呢？**只要我们提前将Closure对象封装成协程即可**，而不是等到需要调度时再封装。那这样处理的话，有什么好处吗？好处是只需要一个任务列表就可以保存所有的任务。但，坏处是什么呢？

依我拙见，我发现，在添加任务时其实是需要加锁的，因为外部多个线程都可以向同一个调度器添加任务，因此必须要加锁。为了减少临界区的大小，没必要现在就立刻把Closure封装为一个协程。上面提到，在调度循环中Closure会被封装成一个协程，这时候并不用担心多线程竞争的问题，因为调度循环只会管理本线程的资源。

### 协程与协程池

我们可以从调度器实现类SchedulerImpl中发现协程池的存在，并且在new\_coroutine中使用到了协程池。

```c++
class SchedulerImpl : public co::Scheduler {
  private:
  
    ...
    
    // pop a Coroutine from the pool
    // 这里就是前文提到的协程对Closure对象的封装
    Coroutine* new_coroutine(Closure* cb) {
        // 从协程池取出一个协程
        Coroutine* co = _co_pool.pop();
        // 保存Closure对象
        co->cb = cb;
        ...
        return co;
    }
    
    ...
    
  private: 
    ...
    // 协程池
    Copool _co_pool;
    
    ...
}
```

协程的结构

```c++
struct Coroutine {
    Coroutine() { memset(this, 0, sizeof(*this)); }
    ~Coroutine() { it.~timer_id_t(); stack.~fastream(); }

    uint32 id;         // coroutine id 协程 id
    uint8 state;       // coroutine state 协程状态
    uint8 sid;         // stack id 共享栈 id
    uint16 _00_;       // reserved
    void* waitx;       // wait info 等待上下文
    // 协程栈的上下文，指向栈底
    tb_context_t ctx;  // context, a pointer points to the stack bottom
  
    // for saving stack data of this coroutine
    // 协程栈 - 私有栈，协程挂起后，栈数据需要从共享栈移入私有栈
    union { fastream stack; char _dummy1[sizeof(fastream)]; };
    
    // 定时任务相关的信息
    union { timer_id_t it;  char _dummy2[sizeof(timer_id_t)]; };
    
    // 一个协程生命周期内只负责一个函数调用，只要Closure被调用了，那么Closure就可以不需要了
    // 因此这部分空间可以留给调度器指针
    // Once the coroutine starts, we no longer need the cb, and it can
    // be used to store the Scheduler pointer.
    union {
        Closure* cb;   // coroutine function
        Scheduler* s;  // scheduler this coroutine runs in
    };
};
```

协程池的实现

```c++
// pool of Coroutine, using index as the coroutine id.
class Copool {
  public:
    // _tb(14, 14) can hold 2^28=256M coroutines.
    Copool() : _tb(14, 14), _id(0) {
        // 预留了能保存 2^14 的元素个数的空间
        _ids.reserve(1u << 14);
    }

    ~Copool() {
        for (int i = 0; i < _id; ++i) _tb[i].~Coroutine();
    }
    
    // 弹出一个协程供程序使用
    Coroutine* pop() {
        // 如果有可用的协程 id
        if (!_ids.empty()) {
            // 从表中取出可空闲可用的协程
            auto& co = _tb[_ids.pop_back()];
            co.state = st_init;
            co.ctx = 0;
            co.stack.clear();
            return &co;
        } else {
            // 从表中取出新的协程
            auto& co = _tb[_id];
            co.id = _id++;
            // 共享栈 id，也就是共享栈索引
            co.sid = (uint8)(co.id & 7);
            return &co;
        }
    }
    // 协程已经使用完毕
    void push(Coroutine* co) {
        // 将此协程添加到ids列表中,表明当前协程空闲可用
        _ids.push_back(co->id);
        if (_ids.size() >= 1024) co->stack.reset();
    }

    Coroutine* operator[](size_t i) {
        return &_tb[i];
    }

  private:
    co::table<Coroutine> _tb;  // 固定大小的二维表，协程都储存在这张表里
    co::vector<int> _ids; // 储存空闲可用的协程 id
    int _id;  // 用于协程 id 的分配，id 即为协程在表中的索引
};
```

协程池负责了协程的创建与Closure对象的封装，为之后协程调度提供了基础。本身的池化也提高了性能，减少了协程创建和销毁的开销。

### 共享栈Stack

```c++
// 栈结构体
struct Stack {
    char* p;       // stack pointer 栈指针
    char* top;     // stack top 栈顶
    Coroutine* co; // coroutine owns this stack 拥有栈的协程
};


SchedulerImpl::SchedulerImpl(uint32 id, uint32 sched_num, uint32 stack_size)
    : _wait_ms((uint32)-1), _id(id), _sched_num(sched_num), 
      _stack_size(stack_size), _running(0), _co_pool(), 
      _stop(false), _timeout(false) {
    ...
    
    _stack = (Stack*) calloc(8, sizeof(Stack));
    
    ...
}

```

cocoyaxi使用共享栈作为运行的协程栈。在SchedulerImpl的构造方法中可以找到，每一个调度器（线程）会申请8个栈，每个栈又可以申请默认1M大小的空间，当所有的共享栈都分配好空间时，最多会占8M空间。



从协程池拿到的协程已经分配了一个sid，即栈id，表示需要使用索引为sid的那个栈。在栈结构体中，成员Coroutine\* co会保存上一次使用它的协程指针，在resume方法中，它用于跟resume时的协程比较，判断这个栈是否被其他协程占用，如果被占用，就会发生栈拷贝。在协程yield时，也不会发生栈拷贝。发生拷贝的情况只有一种可能，就是当栈被其他协程占用时。

```c++
void SchedulerImpl::resume(Coroutine* co) {
    tb_context_from_t from;
    Stack* s = &_stack[co->sid];
    _running = co;
    // 当前栈没有分配过空间，申请 1M 大小空间
    if (s->p == 0) {
        s->p = (char*) co::alloc(_stack_size);
        s->top = s->p + _stack_size;
        s->co = co;
    }
    
    if (co->ctx == 0) {
        // 上一次使用此栈的协程与resume的协程不一致
        // 就把栈上的空间copy到之前的协程的私有栈上去
        // 留出空间给当前需要运行的协程用
        // 如果当前协程经常被访问，
        // 这种优化就不用每次进行栈拷贝
        // 减少 copy 的开销
        // lazy-copy 优化
        if (s->co != co) { 
          this->save_stack(s->co); 
          s->co = co; 
        }
        ...
    }
    
    ...
}
```

使用8个栈可以减小协程之间因为共享栈相同而发生栈拷贝的可能。那些执行频率很高的协程，大部分时间不需要栈拷贝。

这种方案极大程度上避免了内存的浪费，做到了用多少占多少。

### 总舵手Epoll

epoll是实现IO多路复用的利器，在异步编程数头等功，而epoll与协程的搭配也是十分巧妙。

cocoyaxi库中，epoll被使用在调度循环中，充分参与协程调度，不但普通的任务可以由epoll协作调度，IO调度也是拿手好戏，（IO调度又是一个需要大篇章也能讲清楚的难点，之后会开新篇介绍，IO调度相关的内容后文会略过）。

先来认识一下Epoll（Linux）的结构

```c++
/**
 * Epoll for Linux 
 *   - We have to consider about that two different coroutines operates on the 
 *     same socket, one for read and one for write. 
 * 
 *     We use data.u64 of epoll_event to store the user data: 
 *       - the higher 32 bits:  id of the coroutine waiting for EV_read. 
 *       - the lower  32 bits:  id of the coroutine waiting for EV_write. 
 * 
 *     When an IO event is present, id in the user data will be used to resume 
 *     the corresponding coroutine.
 */
class Epoll {
  public:
    Epoll(int sched_id);
    ~Epoll();
    
    ...
    
    int wait(int ms) {
        // 相当于 epoll_wait(_ep, _ev, 1024, ms);
        // HOOK 机制也是大篇章
        return CO_RAW_API(epoll_wait)(_ep, _ev, 1024, ms);
    }
    
    // 注册读事件
    bool Epoll::add_ev_read(int fd, int32 co_id);
    
    // write one byte to the pipe to wake up the epoll.
    // 向管道写端写入一个字节，用于唤醒 epoll
    // 构造函数中管道读端注册读事件到 epoll
    // 如果外部调用了signal，读端fd就会有事件响应，epoll_wait检测到之后就会返回
    // 这样调度循坏就不会阻塞在epoll_wait中，能够调度协程事件，这就是所谓的唤醒
    void signal(char c = 'x') {
        if (atomic_compare_swap(&_signaled, false, true) == false) {
            const int r = (int) CO_RAW_API(write)(_pipe_fds[1], &c, 1);
            ELOG_IF(r != 1) << "pipe write error..";
        }
    }

    const epoll_event& operator[](int i)   const { return _ev[i]; }
    int user_data(const epoll_event& ev)         { return ev.data.fd; }
    // 是否为管道读端
    bool is_ev_pipe(const epoll_event& ev) const { return ev.data.fd == _pipe_fds[0]; }
    // 处理管道读端接收到的事件
    void handle_ev_pipe();
    void close();

  private:
    int _ep;  // epoll fd
    int _pipe_fds[2];  // 管道，用于唤醒epoll
    int _sched_id;  // 调度器 id
    epoll_event* _ev;
    bool _signaled; // 是否唤醒
};

// Epoll 构造函数
Epoll::Epoll(int sched_id) : _sched_id(sched_id), _signaled(false) {
    _ep = epoll_create(1024);
    
    ...

    // register ev_read for _pipe_fds[0] to this epoll.
    co::set_nonblock(_pipe_fds[0]);
    // 给管道读端注册读事件
    CHECK(this->add_ev_read(_pipe_fds[0], 0));
    
    _ev = (epoll_event*) calloc(1024, sizeof(epoll_event));
}

// 管道读端处理读事件
void Epoll::handle_ev_pipe() {
    int32 dummy;
    while (true) {
        int r = CO_RAW_API(read)(_pipe_fds[0], &dummy, 4);
        if (r != -1) {
            if (r < 4) break;
            continue;
        } else {
            if (errno == EWOULDBLOCK || errno == EAGAIN) break;
            if (errno == EINTR) continue;
            ELOG << "pipe read error: " << co::strerror() << ", fd: " << _pipe_fds[0];
            break;
        }
    }
    atomic_swap(&_signaled, false);
}
```

发现没有，Epoll的signal方法其实在添加任务时就被调用了，用于唤醒Epoll，让调度循环能够处理协程调度，处理新添加的任务。

```c++
    void add_new_task(Closure* cb) {
        // 添加新任务
        _task_mgr.add_new_task(cb);
        // 唤醒 epoll
        _epoll->signal();
    }
```

### 进入调度循环

前文提到，调度器被创建后，SchedulerManager会调用调度器的start方法，在调度器的start方法中，会启动一个线程，开启调度循环。

```c++
class SchedulerImpl : public co::Scheduler {
 public:
  ...
  void start() { 
    // 启动一个线程
    Thread(&SchedulerImpl::loop, this).detach(); 
  }
  ...
 private:
  // 调度循环
  void loop();
}

```

调度循环则是调度器的核心，体现了其如何控制协程的执行。

```c++
void SchedulerImpl::loop() {
    // thread local 全局调度器
    gSched = this;
    // 从TaskManager中取出来的 new_tasks
    co::vector<Closure*> new_tasks;
    // 从TaskManager中取出来的 ready_tasks
    co::vector<Coroutine*> ready_tasks;

    while (!_stop) {
        int n = _epoll->wait(_wait_ms);
        if (_stop) break;

        ...

        for (int i = 0; i < n; ++i) {
            //  取出 epoll 事件
            auto& ev = (*_epoll)[i];
            // 判断是否为管道读端，即判断是否为唤醒事件
            if (_epoll->is_ev_pipe(ev)) {
                _epoll->handle_ev_pipe();
                continue;
            }
            
            ...
            // 省略 IO 调度
        }

        CO_DBG_LOG << "> check tasks ready to resume..";
        do {
            // 取出任务列表
            _task_mgr.get_all_tasks(new_tasks, ready_tasks);

            if (!new_tasks.empty()) {
                CO_DBG_LOG << ">> resume new tasks, num: " << new_tasks.size();
                for (size_t i = 0; i < new_tasks.size(); ++i) {
                    // 将Closure对象封装为协程，开始调度协程
                    this->resume(this->new_coroutine(new_tasks[i]));
                }
                // 执行完所有任务后，清理任务
                new_tasks.clear();
            }

            if (!ready_tasks.empty()) {
                CO_DBG_LOG << ">> resume ready tasks, num: " << ready_tasks.size();
                for (size_t i = 0; i < ready_tasks.size(); ++i) {
                    // 这些任务本身就是协程，是那些曾经由于各种原因 yield，
                    // 不过现在资源获取到之后又被添加到ready_tasks列表的协程
                    this->resume(ready_tasks[i]);
                }
                // 执行完所有任务后，清理任务
                ready_tasks.clear();
            }
        } while (0);

        ... // 省略定时任务

        if (_running) _running = 0;
    }

    ...
}
```

到这里，我们可以对协程调度稍作总结：

*   SchedulerManager是一个静态全局变量，它管理所有的调度器，添加任务时会“随机”选择一个调度器，向此调度器的新任务列表添加任务。

*   调度器在被创建之后，会开启新线程，进入调度循环。通过Epoll来协作实现调度控制。



cocoyaxi库的调度器除了以上这些设计之外，还有很多我暂时忽略（脑子不够用）的设计，例如：

*   定时器设计

*   IO任务的调度

*   协程上下文切换

*   HOOK机制

*   ...

这些设计同样值得深入了解，未来会继续深入研究，先开个坑。





注：以上仅是我个人的观点和看法，如有错误与瑕疵，还请指正。

